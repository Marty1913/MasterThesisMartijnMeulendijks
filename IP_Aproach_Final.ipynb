{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2948dfd-c889-47c4-ab8a-b33c3a0e13c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Load in all libraries\n",
    "\n",
    "from azure.identity import AzureCliCredential\n",
    "from azure.storage.filedatalake import DataLakeFileClient\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import re\n",
    "from pulp import LpProblem, LpMinimize, LpMaximize, LpVariable, lpSum, LpStatus, value, LpBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df275ac3-4903-4a1c-9088-97b759e28ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Azure functions\n",
    "\n",
    "def get_credential():\n",
    "    # Haal het token op wat met az login is aangemaakt\n",
    "    return AzureCliCredential()\n",
    "\n",
    "\n",
    "def read_file(credential, storage_account, container,\n",
    "                  filepath):\n",
    "    account_url = \"https://{}.dfs.core.windows.net\".format(storage_account)\n",
    "\n",
    "    file_client = DataLakeFileClient(account_url=account_url,\n",
    "                                     file_system_name=container,\n",
    "                                     file_path=filepath,\n",
    "                                     credential=credential)\n",
    "\n",
    "    downloaded_bytes = io.BytesIO(file_client.download_file().readall())\n",
    "    return downloaded_bytes\n",
    "\n",
    "\n",
    "def write_file(buffer, credential, storage_account, container,\n",
    "                filepath):\n",
    "    account_url = \"https://{}.dfs.core.windows.net\".format(storage_account)\n",
    "\n",
    "    file_client = DataLakeFileClient(account_url=account_url,\n",
    "                                     file_system_name=container,\n",
    "                                     file_path=filepath,\n",
    "                                     credential=credential)\n",
    "\n",
    "    file_client.upload_data(buffer.getvalue(), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcb10cb-14e6-4bd3-a8df-b780024bae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request token\n",
    "credential = get_credential()\n",
    "\n",
    "# Definieer storage-account en containername\n",
    "storage_account_name = \"prda007itweup01dapsts01\"\n",
    "container_name = \"ait-analytics30-s\"\n",
    "account_url = \"https://{}.dfs.core.windows.net\".format(storage_account_name)\n",
    "reports_folder = \"426_rapportage/\"\n",
    "schedule_folder = \"werkvoorraad_planning/\"\n",
    "schedule2_folder = \"werkverdeling_acceptatie/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c38966-dfb5-409e-9980-9c471cf5e2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def get_df(folder = \"426_rapportage/\", file_name = \"426_werkvoorraad_MO_20230131_AMB.xlsx\"):\n",
    "    file_path = folder + '/' + file_name\n",
    "\n",
    "    file_bytes = read_file(credential=credential,\n",
    "                  storage_account=storage_account_name,\n",
    "                  container=container_name,\n",
    "                  filepath=file_path)\n",
    "    \n",
    "    return(file_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb308524-5b4e-4d99-8659-02da9c55216f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove prefix\n",
    "def remove_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text\n",
    "\n",
    "#Function to retrieve all file names from a folder\n",
    "def get_all_file_names(account_url, credential, container_name, folder):\n",
    "    # Get a reference to the file system and directory\n",
    "    service_client = DataLakeServiceClient(account_url=account_url, credential=credential)\n",
    "    file_system_client = service_client.get_file_system_client(container_name)\n",
    "\n",
    "    file_list = []\n",
    "    # List files in the directory\n",
    "    files = file_system_client.get_paths()\n",
    "    for file in files:\n",
    "        if ((file.name.startswith(folder)) & (\".xlsx\" in file.name)):\n",
    "            file_list.append(file.name)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f349ff-1477-40af-9139-689a98d99481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract filename with the specific date\n",
    "def extract_filename_with_date(filenames, date):\n",
    "    for filename in filenames:\n",
    "        if date in filename:\n",
    "            return filename\n",
    "    return None  # Return None if no matching filename is found\n",
    "\n",
    "# Function to convert YYYYMMDD to DD-MM-YYYY\n",
    "def convert_date_format(yyyymmdd):\n",
    "    return f\"{yyyymmdd[6:8]}-{yyyymmdd[4:6]}-{yyyymmdd[0:4]}\"\n",
    "\n",
    "# Function to extract filename with the specific date, considering the new format\n",
    "def extract_filename_with_converted_date(filenames, date):\n",
    "    for filename in filenames:\n",
    "        # Use regular expressions to find the date in YYYYMMDD format within the filename\n",
    "        match = re.search(r'\\d{8}', filename)\n",
    "        if match:\n",
    "            date_in_file = match.group(0)  # Extract the date string\n",
    "            converted_date = convert_date_format(date_in_file)  # Convert to DD-MM-YYYY\n",
    "            if converted_date == date:\n",
    "                return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19958f9b-f740-4fef-9d36-d2de86881186",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data into environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6812d84a-4c06-4326-8a2b-168ae02b31cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create an aggregated dataframe of the employee availability.\n",
    "def transform_schedule(df, skill_lookup):\n",
    "    weekly_schedule_df = df.copy()\n",
    "\n",
    "    # Identifying team names and their respective employee columns\n",
    "    team_columns = [col for col in weekly_schedule_df.columns if 'Team' in col]\n",
    "\n",
    "    # Mapping each team to its members\n",
    "    team_member_map = {}\n",
    "    for i, team_col in enumerate(team_columns):\n",
    "        team_idx = weekly_schedule_df.columns.get_loc(team_col)\n",
    "        if i < len(team_columns) - 1:\n",
    "            next_team_idx = weekly_schedule_df.columns.get_loc(team_columns[i + 1])\n",
    "            members = weekly_schedule_df.columns[team_idx + 1:next_team_idx]\n",
    "        else:\n",
    "            members = weekly_schedule_df.columns[team_idx + 1:]\n",
    "\n",
    "        team_member_map[weekly_schedule_df[team_col].iloc[0]] = members\n",
    "\n",
    "    # Transforming the dataframe\n",
    "    new_data = []\n",
    "    for index, row in weekly_schedule_df.iterrows():\n",
    "        for team_name, team_members in team_member_map.items():\n",
    "            total_hours = row[team_members].apply(pd.to_numeric, errors='coerce').sum()\n",
    "            all_skills_hours = 0\n",
    "            not_all_skills_hours = 0\n",
    "\n",
    "            for member in team_members:\n",
    "                first_name = member\n",
    "                hours = pd.to_numeric(row[first_name], errors='coerce')\n",
    "                if pd.isna(hours):\n",
    "                    hours = 0  # Set NaN values to 0\n",
    "                skill = skill_lookup.get(first_name, 'Acceptant A')  # Default to 'Acceptant A' if not found\n",
    "                \n",
    "                if skill == 'Acceptant B':\n",
    "                    not_all_skills_hours += hours\n",
    "                else:\n",
    "                    all_skills_hours += hours\n",
    "\n",
    "            new_data.append({\n",
    "                'Datum': row['Datum'],\n",
    "                'Dag': row['Dag'],\n",
    "                'Team': team_name,\n",
    "                'TotalHours': total_hours,\n",
    "                'AllSkillsHours': all_skills_hours,\n",
    "                'NotAllSkillsHours': not_all_skills_hours\n",
    "            })\n",
    "\n",
    "    # Create the new dataframe\n",
    "    transformed_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Display the first few rows of the new dataframe\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44b92b-07b8-4f41-8c38-7fd58b9d0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in skill matrix and make lookup file\n",
    "skill_matrix = pd.read_excel(\"Skillmatrix.xlsx\", skiprows=2)\n",
    "\n",
    "def extract_first_name(full_name):\n",
    "    return full_name.split()[0]\n",
    "\n",
    "# Step 1: Extract first names and create a mapping from first name to full name and skill\n",
    "skill_matrix['FirstName'] = skill_matrix['Naam'].apply(extract_first_name)\n",
    "\n",
    "# Step 2: Create a dictionary for skill lookup\n",
    "skill_lookup = skill_matrix.set_index('FirstName')['Medewerkerprofiel'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b52271-e736-4925-9369-5ebffb4de8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get latest working schedule\n",
    "schedule_file_names = get_all_file_names(account_url, credential, container_name, folder = schedule_folder)\n",
    "schedule_file_names2 = get_all_file_names(account_url, credential, container_name, folder = schedule2_folder)\n",
    "\n",
    "weekly_schedules = []\n",
    "for i in schedule_file_names:\n",
    "    split_name = i.rsplit('/',1)\n",
    "    excel_file = get_df(folder=split_name[0] + '/', file_name=split_name[1])\n",
    "    # Check if the 'ROOSTER' sheet exists in the Excel file\n",
    "    if 'ROOSTER' in pd.ExcelFile(excel_file).sheet_names:\n",
    "        temp = pd.read_excel(excel_file, sheet_name='ROOSTER')\n",
    "        temp['Datum'] = pd.to_datetime(temp['Datum'], errors='coerce')\n",
    "        temp = temp.dropna(subset=['Datum'])\n",
    "        temp = temp.dropna(axis=1, how='all')\n",
    "        transformed_temp = transform_schedule(temp, skill_lookup)\n",
    "        weekly_schedules.append(transformed_temp)\n",
    "        # weekly_schedules.append(temp)\n",
    "    else:\n",
    "        print(f\"Skipping file {i} because 'ROOSTER' sheet doesn't exist.\")\n",
    "\n",
    "for i in schedule_file_names2:\n",
    "    split_name = i.rsplit('/',1)\n",
    "    excel_file = get_df(folder=split_name[0] + '/', file_name=split_name[1])\n",
    "    # Check if the 'ROOSTER' sheet exists in the Excel file\n",
    "    if 'ROOSTER' in pd.ExcelFile(excel_file).sheet_names:\n",
    "        temp = pd.read_excel(excel_file, sheet_name='ROOSTER')\n",
    "        temp['Datum'] = pd.to_datetime(temp['Datum'], errors='coerce')\n",
    "        temp = temp.dropna(subset=['Datum'])\n",
    "        temp = temp.dropna(axis=1, how='all')\n",
    "        transformed_temp = transform_schedule(temp, skill_lookup)\n",
    "        weekly_schedules.append(transformed_temp)\n",
    "    else:\n",
    "        print(f\"Skipping file {i} because 'ROOSTER' sheet doesn't exist.\")        \n",
    "\n",
    "schedule = pd.concat(weekly_schedules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b1c9c-0fa1-4e68-858d-7b198f8839a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get task groups\n",
    "task_groups = pd.read_excel('Procesgroep taken acceptatie.xlsx', sheet_name ='Lijstvorm')\n",
    "task_groups = task_groups[['Procesgroep', 'Proces']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808428a9-d2f8-4758-9805-0828a808d72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get backlog of all tasks\n",
    "report_file_names = get_all_file_names(account_url, credential, container_name, folder = reports_folder)\n",
    "\n",
    "reports = []\n",
    "for i in report_file_names:\n",
    "    split_name = i.rsplit('/',1)\n",
    "    excel_file = get_df(folder=split_name[0] + '/', file_name=split_name[1])\n",
    "    date = split_name[1].rsplit('MO_', 1)[1].rsplit('_A')[0]\n",
    "    temp = pd.read_excel(excel_file)\n",
    "    temp['datum'] = pd.to_datetime(split_name[1].rsplit('MO_', 1)[1].rsplit('_A')[0], format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "    reports.append(temp)\n",
    "\n",
    "backlog = pd.concat(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a97deb-77cd-484c-9c99-f1fa6331b9ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff34665-5dd2-4edc-ba4a-53abfc01b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean working backlog\n",
    "backlog = backlog[backlog['contractnummer'].notna()]\n",
    "backlog['contractnummer'] =  backlog['contractnummer'].astype(np.int64)\n",
    "backlog['datum'] = pd.to_datetime(backlog['datum'])\n",
    "backlog = backlog.merge(task_groups, left_on='taaknaam', right_on='Proces', how='left')\n",
    "filtered_backlog = backlog.copy()\n",
    "# filtered_backlog = backlog[['datum', 'contractnummer', 'Procesgroep_y', 'taaknaam', 'taakomschrijving_aangepast', 'teamcode', 'Uitvoeren voor']]\n",
    "filtered_backlog.rename(columns={'Procesgroep_y': 'Procesgroep'}, inplace=True)\n",
    "filtered_backlog = filtered_backlog[filtered_backlog['Procesgroep'] != 'Quion']\n",
    "filtered_backlog['week_nummer'] = filtered_backlog['datum'].dt.isocalendar().week\n",
    "\n",
    "# Define the list of team codes you want to keep\n",
    "team_codes_to_keep = ['Zuid', 'Midden', 'Noord', 'IMD']\n",
    "\n",
    "# Use the isin method to filter the DataFrame\n",
    "filtered_backlog = filtered_backlog[filtered_backlog['teamcode'].isin(team_codes_to_keep)]\n",
    "filtered_backlog.loc[filtered_backlog['productlijn'] == 'Attens Hypotheek', 'teamcode'] = 'Attens'\n",
    "filtered_backlog = filtered_backlog[filtered_backlog[\"teamcode\"] != \"IMD\"]\n",
    "# Group by 'procesgroep' and 'contractnummer', then count unique 'taaknaam' for each group.\n",
    "unique_tasks_per_group_and_contract = filtered_backlog.groupby(['Procesgroep', 'contractnummer'])['Proces'].nunique().reset_index(name='unique_tasks')\n",
    "\n",
    "# Now, group by 'procesgroep' again to calculate the average number of unique tasks per procesgroep.\n",
    "average_unique_tasks_per_procesgroep = unique_tasks_per_group_and_contract.groupby('Procesgroep')['unique_tasks'].mean()\n",
    "\n",
    "# Define the column names\n",
    "column_names = ['Procesgroep', 'Normtijd (in minuten)']\n",
    "\n",
    "# Manually enter the rows with data\n",
    "rows = [\n",
    "    [\"Aanvragen\", 16.8],\n",
    "    [\"Rebound\", 21.0],\n",
    "    [\"1e fiat\", 55.8],\n",
    "    [\"2e fiat\", 27.0],\n",
    "    ['Afronding dossier', 10.0/2],\n",
    "    ['Schoningstaken', 10.0]\n",
    "]\n",
    "\n",
    "# Create the dataframe\n",
    "task_times = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "filtered_backlog = filtered_backlog.merge(task_times, left_on=\"Procesgroep\", right_on=\"Procesgroep\", how=\"left\")\n",
    "filtered_backlog['Normtijd (in minuten)'] = (filtered_backlog['Normtijd (in minuten)'] / 60).round(2)\n",
    "filtered_backlog.rename(columns={'Normtijd (in minuten)': 'Normtijd (in hours)'}, inplace=True)\n",
    "filtered_backlog['datum'] = filtered_backlog['datum'] + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328e00b-33e8-4547-80d3-4f121fc1e143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Clean schedule\n",
    "schedule = schedule.drop_duplicates()\n",
    "schedule = schedule.fillna(0)\n",
    "\n",
    "# List of columns to exclude from replacement\n",
    "exclude_columns = ['Datum', 'Dag', 'Team']\n",
    "\n",
    "# List of columns representing employee hours\n",
    "employee_columns = [col for col in schedule.columns if col not in exclude_columns]\n",
    "\n",
    "# Apply the replacement only to the employee columns\n",
    "schedule[employee_columns] = schedule[employee_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "team_codes_to_keep = ['Zuid', 'Midden', 'Noord', 'Attens']\n",
    "schedule = schedule[schedule['Team'].isin(team_codes_to_keep)]\n",
    "\n",
    "schedule = schedule.drop_duplicates(subset=['Datum', 'Dag', 'Team'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec6849-10cb-485c-a59f-1c0556404ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solveIP(filtered_backlog, schedule, start_date_str, end_date_str):\n",
    "    # Define the date range for the analysis\n",
    "    start_date = datetime.strptime(start_date_str, \"%d-%m-%Y\")\n",
    "    end_date = datetime.strptime(end_date_str, \"%d-%m-%Y\")\n",
    "\n",
    "    # Filter the dataframes\n",
    "    filtered_backlog_partly = filtered_backlog[(filtered_backlog['datum'] >= start_date) & (filtered_backlog['Uitvoeren voor'] <= end_date)]\n",
    "    schedule_partly = schedule[(schedule['Datum'] >= start_date) & (schedule['Datum'] <= end_date)]\n",
    "    dates = pd.date_range(start=schedule_partly[\"Datum\"].min(), end=schedule_partly[\"Datum\"].max(), freq='D')\n",
    "\n",
    "    # Initialize the model\n",
    "    model = LpProblem(\"Workforce_Scheduling\", LpMaximize)\n",
    "\n",
    "    # Decision Variables\n",
    "    filtered_backlog_partly['case_number_task'] = filtered_backlog_partly['contractnummer'].astype(str) + '_' + filtered_backlog_partly['taaknaam'].astype(str) + '_' + filtered_backlog_partly['Taakgroep'].astype(str)\n",
    "    filtered_backlog_partly['case_number_task'] = filtered_backlog_partly['case_number_task'].apply(lambda x: x.replace(\" \", \"_\").replace(\"/\", \"_\"))\n",
    "\n",
    "    all_tasks = filtered_backlog_partly['case_number_task'].unique()\n",
    "    x = LpVariable.dicts(\"x\", [(i, k) for i in all_tasks for k in dates], cat='Binary')\n",
    "\n",
    "    # Objective Function: Maximize the number of tasks completed\n",
    "    model += lpSum(x[i, k] for i in all_tasks for k in dates)\n",
    "\n",
    "    # Constraint: Each task must be assigned exactly once\n",
    "    for i in all_tasks:\n",
    "        model += lpSum(x[i, k] for k in dates) == 1\n",
    "\n",
    "    # Constraint: No deadlines are crossed\n",
    "    for i in all_tasks:\n",
    "        deadline = filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == i, 'Uitvoeren voor'].iloc[0]\n",
    "        model += lpSum(x[i, k] for k in dates if k <= deadline) == 1    \n",
    "\n",
    "    # Constraint: Total time cost of tasks assigned to a team on a date does not exceed the available hours\n",
    "    for j in schedule_partly['Team'].unique():\n",
    "        for k in dates:\n",
    "            total_hours_available = schedule_partly[(schedule_partly['Datum'] == k) & (schedule_partly['Team'] == j)]['TotalHours'].values[0]\n",
    "            all_skills_hours_available = schedule_partly[(schedule_partly['Datum'] == k) & (schedule_partly['Team'] == j)]['AllSkillsHours'].values[0]\n",
    "            not_all_skills_hours_available = schedule_partly[(schedule_partly['Datum'] == k) & (schedule_partly['Team'] == j)]['NotAllSkillsHours'].values[0]\n",
    "\n",
    "            sum_tasks = lpSum(filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == i, 'Normtijd (in hours)'].iloc[0] * x[i, k] \n",
    "                              for i in all_tasks if filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == i, 'teamcode'].iloc[0] == j)\n",
    "\n",
    "            model += sum_tasks <= total_hours_available\n",
    "\n",
    "            # Constraint for tasks requiring all skills\n",
    "            sum_all_skills_tasks = lpSum(filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == i, 'Normtijd (in hours)'].iloc[0] * x[i, k]\n",
    "                                         for i in all_tasks if filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == i, 'teamcode'].iloc[0] == j and\n",
    "                                         filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == i, 'Procesgroep'].iloc[0] == '2e fiat')\n",
    "            model += sum_all_skills_tasks <= all_skills_hours_available\n",
    "\n",
    "            # Constraint for other tasks that can use either type of skill hours\n",
    "            sum_other_tasks = lpSum(filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == i, 'Normtijd (in hours)'].iloc[0] * x[i, k]\n",
    "                                    for i in all_tasks if filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == i, 'teamcode'].iloc[0] == j and\n",
    "                                    filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == i, 'Procesgroep'].iloc[0] != '2e fiat')\n",
    "            model += sum_other_tasks <= (all_skills_hours_available + not_all_skills_hours_available)\n",
    "\n",
    "    model.solve()\n",
    "\n",
    "\n",
    "    def split_string(input_string):\n",
    "        # Extract the part within single quotes\n",
    "        part1 = re.search(r\"'([^']*)'\", input_string)\n",
    "        if part1:\n",
    "            part1 = part1.group(1)\n",
    "\n",
    "        # Extract the timestamp part and convert it to datetime\n",
    "        part2 = re.search(r\"Timestamp\\('([^']+)'\\)\", input_string)\n",
    "        if part2:\n",
    "            timestamp_str = part2.group(1)\n",
    "            # Parse the datetime from the string\n",
    "            part2 = datetime.strptime(timestamp_str, '%Y_%m_%d_%H:%M:%S')\n",
    "\n",
    "        return part1, part2\n",
    "\n",
    "    # Extracting the assignments\n",
    "    assignments = []\n",
    "    for v in model.variables():\n",
    "        if v.varValue > 0.99:  # Filter to only include assigned tasks\n",
    "            task, date = split_string(str(v))\n",
    "            team = filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == task, 'teamcode'].iloc[0]\n",
    "            hours = filtered_backlog_partly.loc[filtered_backlog_partly['case_number_task'] == task, 'Normtijd (in hours)'].iloc[0]\n",
    "            assignments.append((task, team, date, hours))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    assignments_df = pd.DataFrame(assignments, columns=['Task', 'Team', 'Date', 'Hours'])\n",
    "    assignments_df['Scheduled'] = 'Yes'\n",
    "\n",
    "    # Create a DataFrame of all tasks with their deadlines\n",
    "    all_tasks_df = filtered_backlog_partly[['case_number_task', 'Uitvoeren voor']]\n",
    "    all_tasks_df.columns = ['Task', 'Deadline']\n",
    "    all_tasks_df = all_tasks_df.drop_duplicates()\n",
    "\n",
    "    # Merge to see which tasks were scheduled and which were not\n",
    "    result_df = pd.merge(all_tasks_df, assignments_df, on='Task', how='left')\n",
    "    result_df.fillna({'Scheduled': 'No', 'Date': 'Not Scheduled'}, inplace=True)\n",
    "\n",
    "    # Ensure the task identifier columns are named consistently\n",
    "    result_df.rename(columns={'Task': 'case_number_task', 'Team': 'teamcode', 'Hours': 'Normtijd (in hours)'}, inplace=True)\n",
    "\n",
    "    # Perform a left merge to match tasks and bring the 'Team' and 'Hours' columns\n",
    "    merge_df = pd.merge(result_df, filtered_backlog_partly[['case_number_task', 'teamcode', 'Normtijd (in hours)']],\n",
    "                        on='case_number_task', how='left', suffixes=('', '_from_backlog'))\n",
    "\n",
    "    # Fill NaN values in 'Team' and 'Hours' in result_df with the values from merge_df\n",
    "    result_df['teamcode'] = result_df['teamcode'].fillna(merge_df['teamcode_from_backlog'])\n",
    "    result_df['Normtijd (in hours)'] = result_df['Normtijd (in hours)'].fillna(merge_df['Normtijd (in hours)_from_backlog'])\n",
    "\n",
    "    # Drop the extra columns from merge_df if they exist\n",
    "    result_df.drop(columns=['teamcode_from_backlog', 'Normtijd (in hours)_from_backlog'], errors='ignore', inplace=True)\n",
    "    \n",
    "    # Check for duplicate task assignments\n",
    "    duplicates = result_df[result_df.duplicated(['case_number_task', 'Date'], keep=False)]\n",
    "    if not duplicates.empty:\n",
    "        print(\"Duplicate task assignments detected:\")\n",
    "        print(duplicates)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa765d0f-837d-45cc-851a-ede24713215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your start and end date for the date range\n",
    "start_date = '2024-02-19'\n",
    "end_date = '2023-03-01'\n",
    "\n",
    "# Filter the dates within the specified range and exclude weekends\n",
    "filtered_dates = filtered_backlog[\n",
    "    (filtered_backlog['datum'] >= start_date) & \n",
    "    (filtered_backlog['datum'] <= end_date) &\n",
    "    (filtered_backlog['datum'].dt.weekday < 5)  # Monday=0, Sunday=6\n",
    "][\"datum\"].unique()\n",
    "\n",
    "# Create an array of dates in the \"yyyy-mm-dd\" format\n",
    "date_strings = filtered_dates.strftime('%d-%m-%Y').tolist()\n",
    "\n",
    "# Initialize an empty list to hold DataFrames\n",
    "dfs = []\n",
    "\n",
    "end_date = '01-03-2024'\n",
    "\n",
    "# Loop over each date in the string format\n",
    "for start_date_str in date_strings:\n",
    "    # Call the solveIP function\n",
    "    res = solveIP(filtered_backlog, schedule, start_date_str, end_date)\n",
    "    \n",
    "    # Append the result to the list of DataFrames\n",
    "    dfs.append(res)\n",
    "\n",
    "# Concatenate all the DataFrames into one big DataFrame\n",
    "results_df = pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2fe43c-950f-46b9-8295-c560dc5d2fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Separate scheduled and not scheduled tasks\n",
    "scheduled_df = results_df[results_df.Date != \"Not Scheduled\"]\n",
    "not_scheduled_df = results_df[results_df.Date == \"Not Scheduled\"]\n",
    "\n",
    "# Process scheduled tasks\n",
    "scheduled_df = scheduled_df.sort_values('Date').drop_duplicates('case_number_task', keep='last')\n",
    "scheduled_df['Date'] = pd.to_datetime(scheduled_df['Date'])\n",
    "daily_team_hours = scheduled_df.groupby(['teamcode', 'Date'])['Normtijd (in hours)'].sum().reset_index()\n",
    "\n",
    "# Process not scheduled tasks\n",
    "not_scheduled_df['Deadline'] = pd.to_datetime(not_scheduled_df['Deadline'])\n",
    "not_scheduled_hours_per_day = not_scheduled_df.groupby(['teamcode', 'Deadline'])['Normtijd (in hours)'].sum().reset_index()\n",
    "\n",
    "# Filter the schedule hours\n",
    "start_date = daily_team_hours['Date'].min()\n",
    "end_date = '2023-03-01'\n",
    "schedule_hours = schedule[\n",
    "    (schedule['Datum'] >= start_date) & \n",
    "    (schedule['Datum'] <= end_date) &\n",
    "    (schedule['Datum'].dt.weekday < 5)  # Monday=0, Sunday=6\n",
    "]\n",
    "schedule_hours = schedule_hours[['Datum', \"Team\", \"TotalHours\"]].rename(columns={'Datum': 'Date', 'Team': 'teamcode'})\n",
    "\n",
    "# Merge scheduled tasks with schedule hours\n",
    "plot_data = pd.merge(daily_team_hours, schedule_hours, on=['teamcode', 'Date'], how='outer')\n",
    "plot_data = plot_data.sort_values(by=[\"teamcode\", \"Date\"])\n",
    "\n",
    "# Create a plot for each team\n",
    "teams = plot_data['teamcode'].unique()\n",
    "\n",
    "for team in teams:\n",
    "    team_schedule = schedule_hours[schedule_hours['teamcode'] == team].copy()  # Make a copy here    \n",
    "    team_scheduled = daily_team_hours[daily_team_hours['teamcode'] == team]\n",
    "    team_unscheduled = not_scheduled_hours_per_day[not_scheduled_hours_per_day['teamcode'] == team]\n",
    "    \n",
    "    # Ensure 'Date' in team_schedule is converted to datetime for accurate plotting\n",
    "    team_schedule['Date'] = pd.to_datetime(team_schedule['Date'])\n",
    "\n",
    "    # Create a date range that includes all days, filling the gaps (e.g., weekends)\n",
    "    all_dates = pd.date_range(start=team_schedule['Date'].min(), end=team_schedule['Date'].max())\n",
    "\n",
    "    # Merge scheduled and unscheduled dataframes with all_dates to ensure all dates are included\n",
    "    all_dates_df = pd.DataFrame(all_dates, columns=['Date'])\n",
    "    combined_hours = pd.merge(all_dates_df, team_scheduled, on='Date', how='left')\n",
    "    combined_hours = pd.merge(combined_hours, team_unscheduled, left_on='Date', right_on='Deadline', how='left', suffixes=('_scheduled', '_unscheduled')).fillna(0)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Scheduled task hours\n",
    "    ax.bar(combined_hours['Date'], combined_hours['Normtijd (in hours)_scheduled'], width=0.4, label='Scheduled Hours', align='center', color='blue')\n",
    "\n",
    "    # Unscheduled task hours\n",
    "    ax.bar(combined_hours['Date'], combined_hours['Normtijd (in hours)_unscheduled'], width=0.4, label='Not Scheduled Tasks', align='center', color='orange', bottom=combined_hours['Normtijd (in hours)_scheduled'])\n",
    "\n",
    "    # Available hours with gaps for weekends or non-working days\n",
    "    available_hours_mask = team_schedule['TotalHours'] > 0\n",
    "    ax.plot(team_schedule.loc[available_hours_mask, 'Date'], team_schedule.loc[available_hours_mask, 'TotalHours'], color='red', marker='o', linestyle='-', linewidth=2, markersize=8, label='Available Hours')\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Hours')\n",
    "    ax.set_title(f'Task Schedule for Team: {team}')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Images/HeuristicSchedule_{team}.png')\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environ",
   "language": "python",
   "name": "environ"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
