{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0752e8f2-d014-415b-9398-0319d66b171d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import re\n",
    "import itertools\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "from azure.identity import AzureCliCredential\n",
    "from azure.storage.filedatalake import DataLakeFileClient, DataLakeServiceClient\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import ParameterGrid, TimeSeriesSplit\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16e791f-bc01-48b4-b976-4389f135fcf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_credential():\n",
    "    # Haal het token op wat met az login is aangemaakt\n",
    "    return AzureCliCredential()\n",
    "\n",
    "\n",
    "def read_file(credential, storage_account, container,\n",
    "                  filepath):\n",
    "    account_url = \"https://{}.dfs.core.windows.net\".format(storage_account)\n",
    "\n",
    "    file_client = DataLakeFileClient(account_url=account_url,\n",
    "                                     file_system_name=container,\n",
    "                                     file_path=filepath,\n",
    "                                     credential=credential)\n",
    "\n",
    "    downloaded_bytes = io.BytesIO(file_client.download_file().readall())\n",
    "    return downloaded_bytes\n",
    "\n",
    "\n",
    "def write_file(buffer, credential, storage_account, container,\n",
    "                filepath):\n",
    "    account_url = \"https://{}.dfs.core.windows.net\".format(storage_account)\n",
    "\n",
    "    file_client = DataLakeFileClient(account_url=account_url,\n",
    "                                     file_system_name=container,\n",
    "                                     file_path=filepath,\n",
    "                                     credential=credential)\n",
    "\n",
    "    file_client.upload_data(buffer.getvalue(), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0579f2-fd07-4e5b-86d2-d870f563f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haal het token op wat met az login is aangemaakt\n",
    "credential = get_credential()\n",
    "\n",
    "# Definieer storage-account en containername\n",
    "storage_account_name = \"prda007itweup01dapsts01\"\n",
    "container_name = \"ait-analytics30-s\"\n",
    "account_url = \"https://{}.dfs.core.windows.net\".format(storage_account_name)\n",
    "reports_folder = \"426_rapportage/\"\n",
    "schedule_folder = \"werkvoorraad_planning/\"\n",
    "schedule2_folder = \"werkverdeling_acceptatie/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1094efb-2723-4cf7-a8a0-94b2db48a51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def get_df(folder = \"426_rapportage/\", file_name = \"426_werkvoorraad_MO_20230131_AMB.xlsx\"):\n",
    "    file_path = folder + '/' + file_name\n",
    "\n",
    "    file_bytes = read_file(credential=credential,\n",
    "                  storage_account=storage_account_name,\n",
    "                  container=container_name,\n",
    "                  filepath=file_path)\n",
    "    \n",
    "    return(file_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e85dfb8-f367-424e-abc9-ea2126951330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_all_file_names(account_url, credential, container_name, folder):\n",
    "    # Get a reference to the file system and directory\n",
    "    service_client = DataLakeServiceClient(account_url=account_url, credential=credential)\n",
    "    file_system_client = service_client.get_file_system_client(container_name)\n",
    "\n",
    "    file_list = []\n",
    "    # List files in the directory\n",
    "    files = file_system_client.get_paths()\n",
    "    for file in files:\n",
    "        if ((file.name.startswith(folder)) & (\".xlsx\" in file.name)):\n",
    "            file_list.append(file.name)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcf18eb-ecb2-47b8-8e73-d47948467338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract filename with the specific date\n",
    "def extract_filename_with_date(filenames, date):\n",
    "    for filename in filenames:\n",
    "        if date in filename:\n",
    "            return filename\n",
    "    return None  # Return None if no matching filename is found\n",
    "\n",
    "# Function to convert YYYYMMDD to DD-MM-YYYY\n",
    "def convert_date_format(yyyymmdd):\n",
    "    return f\"{yyyymmdd[6:8]}-{yyyymmdd[4:6]}-{yyyymmdd[0:4]}\"\n",
    "\n",
    "# Function to extract filename with the specific date, considering the new format\n",
    "def extract_filename_with_converted_date(filenames, date):\n",
    "    for filename in filenames:\n",
    "        # Use regular expressions to find the date in YYYYMMDD format within the filename\n",
    "        match = re.search(r'\\d{8}', filename)\n",
    "        if match:\n",
    "            date_in_file = match.group(0)  # Extract the date string\n",
    "            converted_date = convert_date_format(date_in_file)  # Convert to DD-MM-YYYY\n",
    "            if converted_date == date:\n",
    "                return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad4ed6-91b9-4ea3-8bd7-9cb43abb383b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data into environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5cba5e-81d9-4e4f-aebe-f7b0cd59d007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_schedule(df):\n",
    "    weekly_schedule_df = df\n",
    "\n",
    "    # Identifying team names and their respective employee columns\n",
    "    team_columns = [col for col in weekly_schedule_df.columns if 'Team' in col]\n",
    "\n",
    "    # Mapping each team to its members\n",
    "    team_member_map = {}\n",
    "    for i, team_col in enumerate(team_columns):\n",
    "        team_idx = weekly_schedule_df.columns.get_loc(team_col)\n",
    "        if i < len(team_columns) - 1:\n",
    "            next_team_idx = weekly_schedule_df.columns.get_loc(team_columns[i + 1])\n",
    "            members = weekly_schedule_df.columns[team_idx + 1:next_team_idx]\n",
    "        else:\n",
    "            members = weekly_schedule_df.columns[team_idx + 1:]\n",
    "\n",
    "        team_member_map[weekly_schedule_df[team_col].iloc[0]] = members\n",
    "\n",
    "    # Transforming the dataframe\n",
    "    new_data = []\n",
    "    for index, row in weekly_schedule_df.iterrows():\n",
    "        for team_name, team_members in team_member_map.items():\n",
    "            total_hours = row[team_members].apply(pd.to_numeric, errors='coerce').sum()\n",
    "            new_data.append({\n",
    "                'Datum': row['Datum'],\n",
    "                'Dag': row['Dag'],\n",
    "                'Team': team_name,\n",
    "                'TotalHours': total_hours\n",
    "            })\n",
    "\n",
    "    # Create the new dataframe\n",
    "    transformed_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Display the first few rows of the new dataframe\n",
    "    return(transformed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d92e7c-1b5d-4e09-a797-315e482833d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get latest working schedule\n",
    "schedule_file_names = get_all_file_names(account_url, credential, container_name, folder = schedule_folder)\n",
    "schedule_file_names2 = get_all_file_names(account_url, credential, container_name, folder = schedule2_folder)\n",
    "\n",
    "weekly_schedules = []\n",
    "for i in schedule_file_names:\n",
    "    split_name = i.rsplit('/',1)\n",
    "    excel_file = get_df(folder=split_name[0] + '/', file_name=split_name[1])\n",
    "    # Check if the 'ROOSTER' sheet exists in the Excel file\n",
    "    if 'ROOSTER' in pd.ExcelFile(excel_file).sheet_names:\n",
    "        temp = pd.read_excel(excel_file, sheet_name='ROOSTER')\n",
    "        temp['Datum'] = pd.to_datetime(temp['Datum'], errors='coerce')\n",
    "        temp = temp.dropna(subset=['Datum'])\n",
    "        temp = temp.dropna(axis=1, how='all')\n",
    "        transformed_temp = transform_schedule(temp)\n",
    "        weekly_schedules.append(transformed_temp)\n",
    "    else:\n",
    "        print(f\"Skipping file {i} because 'ROOSTER' sheet doesn't exist.\")\n",
    "\n",
    "for i in schedule_file_names2:\n",
    "    split_name = i.rsplit('/',1)\n",
    "    excel_file = get_df(folder=split_name[0] + '/', file_name=split_name[1])\n",
    "    # Check if the 'ROOSTER' sheet exists in the Excel file\n",
    "    if 'ROOSTER' in pd.ExcelFile(excel_file).sheet_names:\n",
    "        temp = pd.read_excel(excel_file, sheet_name='ROOSTER')\n",
    "        temp['Datum'] = pd.to_datetime(temp['Datum'], errors='coerce')\n",
    "        temp = temp.dropna(subset=['Datum'])\n",
    "        temp = temp.dropna(axis=1, how='all')\n",
    "        transformed_temp = transform_schedule(temp)\n",
    "        weekly_schedules.append(transformed_temp)\n",
    "    else:\n",
    "        print(f\"Skipping file {i} because 'ROOSTER' sheet doesn't exist.\")        \n",
    "\n",
    "schedule = pd.concat(weekly_schedules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd3d73b-82a7-4461-9423-ef2136853dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get task groups\n",
    "task_groups = pd.read_excel('Procesgroep taken acceptatie.xlsx', sheet_name ='Lijstvorm')\n",
    "task_groups = task_groups[['Procesgroep', 'Proces']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbb8557-a95d-4bc6-849d-e452a822d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get working backlog\n",
    "report_file_names = get_all_file_names(account_url, credential, container_name, folder = reports_folder)\n",
    "\n",
    "reports = []\n",
    "for i in report_file_names:\n",
    "    split_name = i.rsplit('/',1)\n",
    "    excel_file = get_df(folder=split_name[0] + '/', file_name=split_name[1])\n",
    "    date = split_name[1].rsplit('MO_', 1)[1].rsplit('_A')[0]\n",
    "    temp = pd.read_excel(excel_file)\n",
    "    temp['datum'] = pd.to_datetime(split_name[1].rsplit('MO_', 1)[1].rsplit('_A')[0], format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "    reports.append(temp)\n",
    "\n",
    "backlog = pd.concat(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f75c3-19b3-4410-ae01-77223b2be26e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc53616-6741-45ce-97ee-90089660d014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean working backlog\n",
    "backlog = backlog[backlog['contractnummer'].notna()]\n",
    "backlog['contractnummer'] =  backlog['contractnummer'].astype(np.int64)\n",
    "backlog['datum'] = pd.to_datetime(backlog['datum'])\n",
    "backlog = backlog.merge(task_groups, left_on='taaknaam', right_on='Proces', how='left')\n",
    "filtered_backlog = backlog.copy()\n",
    "# filtered_backlog = backlog[['datum', 'contractnummer', 'Procesgroep_y', 'taaknaam', 'taakomschrijving_aangepast', 'teamcode', 'Uitvoeren voor']]\n",
    "filtered_backlog.rename(columns={'Procesgroep_y': 'Procesgroep'}, inplace=True)\n",
    "filtered_backlog = filtered_backlog[filtered_backlog['Procesgroep'] != 'Quion']\n",
    "filtered_backlog['week_nummer'] = filtered_backlog['datum'].dt.isocalendar().week\n",
    "\n",
    "# Define the list of team codes you want to keep\n",
    "team_codes_to_keep = ['Zuid', 'Midden', 'Noord', 'IMD']\n",
    "\n",
    "# Use the isin method to filter the DataFrame\n",
    "filtered_backlog = filtered_backlog[filtered_backlog['teamcode'].isin(team_codes_to_keep)]\n",
    "filtered_backlog.loc[filtered_backlog['productlijn'] == 'Attens Hypotheek', 'teamcode'] = 'Attens'\n",
    "filtered_backlog = filtered_backlog[filtered_backlog[\"teamcode\"] != \"IMD\"]\n",
    "# Group by 'procesgroep' and 'contractnummer', then count unique 'taaknaam' for each group.\n",
    "unique_tasks_per_group_and_contract = filtered_backlog.groupby(['Procesgroep', 'contractnummer'])['Proces'].nunique().reset_index(name='unique_tasks')\n",
    "\n",
    "# Now, group by 'procesgroep' again to calculate the average number of unique tasks per procesgroep.\n",
    "average_unique_tasks_per_procesgroep = unique_tasks_per_group_and_contract.groupby('Procesgroep')['unique_tasks'].mean()\n",
    "\n",
    "# Define the column names\n",
    "column_names = ['Procesgroep', 'Normtijd (in minuten)']\n",
    "\n",
    "# Manually enter the rows with data\n",
    "rows = [\n",
    "    [\"Aanvragen\", 16.8],\n",
    "    [\"Rebound\", 21.0],\n",
    "    [\"1e fiat\", 55.8],\n",
    "    [\"2e fiat\", 27.0],\n",
    "    ['Afronding dossier', 10.0/2],\n",
    "    ['Schoningstaken', 10.0]\n",
    "]\n",
    "\n",
    "# Create the dataframe\n",
    "task_times = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "filtered_backlog = filtered_backlog.merge(task_times, left_on=\"Procesgroep\", right_on=\"Procesgroep\", how=\"left\")\n",
    "filtered_backlog['Normtijd (in minuten)'] = (filtered_backlog['Normtijd (in minuten)'] / 60).round(2)\n",
    "filtered_backlog.rename(columns={'Normtijd (in minuten)': 'Normtijd (in hours)'}, inplace=True)\n",
    "filtered_backlog['datum'] = filtered_backlog['datum'] + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a0c0bd-a929-4628-9285-313d3006951b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean schedule\n",
    "schedule = schedule.drop_duplicates()\n",
    "schedule = schedule.fillna(0)\n",
    "\n",
    "# List of columns to exclude from replacement\n",
    "exclude_columns = ['Datum', 'Dag', 'Team']\n",
    "\n",
    "# List of columns representing employee hours\n",
    "employee_columns = [col for col in schedule.columns if col not in exclude_columns]\n",
    "\n",
    "# Apply the replacement only to the employee columns\n",
    "schedule[employee_columns] = schedule[employee_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# Remove weekends\n",
    "schedule = schedule[schedule['Dag'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a1076-f345-43fe-be07-fd53791335b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date_str = '19-02-2024'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ec6a8f-9c17-4d03-8da5-92d2891872e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopExecution(Exception):\n",
    "    def _render_traceback_(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cb4ff-0ec6-4957-8afb-c382f2fdd86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_working_days(start_date_str, working_days=10):\n",
    "    start_date = datetime.strptime(start_date_str, '%d-%m-%Y')\n",
    "    end_date = start_date\n",
    "    added_days = 0\n",
    "\n",
    "    while added_days < working_days:\n",
    "        end_date += timedelta(days=1)\n",
    "        if end_date.weekday() < 5:  # Monday to Friday are considered\n",
    "            added_days += 1\n",
    "\n",
    "    return start_date, end_date\n",
    "\n",
    "# Your start date\n",
    "start_date, end_date = add_working_days(start_date_str)\n",
    "\n",
    "# Filtering the DataFrame\n",
    "specific_schedule = schedule[(schedule['Datum'] >= start_date) & (schedule['Datum'] <= end_date)]\n",
    "specific_backlog = filtered_backlog[filtered_backlog['datum'] == (start_date).strftime('%Y-%m-%d')]\n",
    "\n",
    "# Check if the DataFrames are empty and print appropriate messages\n",
    "if specific_schedule.empty and specific_backlog.empty:\n",
    "    print(\"Both 'schedule' and 'backlog' are empty.\")\n",
    "    raise StopExecution\n",
    "elif specific_schedule.empty:\n",
    "    print(\"'schedule' is empty.\")\n",
    "    raise StopExecution\n",
    "elif specific_schedule.iloc[0]['Dag'] == 'zondag':\n",
    "    print(\"Day of the week is sunday.\")\n",
    "    raise StopExecution\n",
    "elif specific_backlog.empty:\n",
    "    print(\"'backlog' is empty.\")\n",
    "    raise StopExecution\n",
    "elif specific_backlog.empty:\n",
    "    print(\"'backlog' is empty.\")\n",
    "    raise StopExecution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fd63b5-8a54-40b0-aa7b-9f1342c2fcc7",
   "metadata": {},
   "source": [
    "## Timeseries analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525cfb9-95e5-4acf-b293-18b2c742ab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'datum' to datetime and make sure 'Normtijd (in hours)' is numeric\n",
    "filtered_backlog['datum'] = pd.to_datetime(filtered_backlog['datum'])\n",
    "filtered_backlog['Normtijd (in hours)'] = pd.to_numeric(filtered_backlog['Normtijd (in hours)'], errors='coerce')\n",
    "\n",
    "# Group by 'teamcode' and 'datum' and sum 'Normtijd (in hours)'\n",
    "workload = filtered_backlog.groupby(['teamcode', 'datum'])['Normtijd (in hours)'].sum().reset_index()\n",
    "\n",
    "# Now set 'datum' as index after sorting the DataFrame\n",
    "workload.sort_values(by='datum', inplace=True)\n",
    "workload.set_index('datum', inplace=True)\n",
    "\n",
    "# Get unique team codes\n",
    "teamcodes = workload['teamcode'].unique()\n",
    "\n",
    "# Analysis for each teamcode\n",
    "for teamcode in teamcodes:\n",
    "    print(f\"Analysis for Team: {teamcode}\")\n",
    "\n",
    "    # Extract data for current team\n",
    "    team_workload = workload[workload['teamcode'] == teamcode]\n",
    "    # We need to make sure that we have a continuous time series, potentially resampling\n",
    "    team_workload_daily = team_workload.resample('D').sum().fillna(0)  # Fill missing dates with 0 workloads\n",
    "    # Seasonal Decomposition\n",
    "    decomposition = seasonal_decompose(team_workload_daily['Normtijd (in hours)'], model='additive')\n",
    "    \n",
    "    # Plotting the decomposition\n",
    "    print(\"Seasonal Decomposition:\")\n",
    "    fig = decomposition.plot()\n",
    "    fig.suptitle(f'Seasonal Decomposition for {teamcode}', fontsize=16)\n",
    "    axes = fig.get_axes()\n",
    "    axes[0].set_title(\"\", fontsize=12)\n",
    "    # plt.title(f'Seasonal Decomposition for {teamcode}')\n",
    "    plt.savefig(f'Images/decomposition_{teamcode}.png')\n",
    "    plt.show() \n",
    "    \n",
    "    # Plot ACF and PACF\n",
    "    print(\"Autocorrelation Function (ACF) & Partial Autocorrelation Function (PACF):\")\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_acf(team_workload_daily['Normtijd (in hours)'], lags=40, alpha=0.05, title=f'ACF for {teamcode}')\n",
    "    plt.savefig(f'Images/ACF_{teamcode}.png')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plot_pacf(team_workload_daily['Normtijd (in hours)'], lags=40, alpha=0.05, title=f'PACF for {teamcode}')\n",
    "    plt.savefig(f'Images/PACF_{teamcode}.png')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca82644-3473-481d-8c17-8924c45c0766",
   "metadata": {},
   "source": [
    "## Add columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbbc064-2715-431c-9ee6-7456315ca4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate total working time per day for each team\n",
    "daily_work = filtered_backlog.groupby(['teamcode', filtered_backlog['datum'].dt.date])['Normtijd (in hours)'].sum().reset_index(name='total_work_hours')\n",
    "daily_work['datum'] = pd.to_datetime(daily_work['datum'])\n",
    "\n",
    "# Pivot the data to have teams as columns and dates as rows\n",
    "pivot_daily_work = daily_work.pivot(index='datum', columns='teamcode', values='total_work_hours').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea1924-d82f-476e-b6f1-bec9cbacdb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from statsmodels.tools.sm_exceptions import ConvergenceWarning\n",
    "warnings.simplefilter('ignore', ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231703bd-b19d-4dda-a27e-da22b6af5398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each Procesgroep per day for each team\n",
    "procesgroep_counts = filtered_backlog.groupby(['teamcode', 'datum', 'Procesgroep']).size().reset_index(name='count')\n",
    "procesgroep_pivot = procesgroep_counts.pivot_table(index=['teamcode', 'datum'], columns='Procesgroep', values='count', fill_value=0).reset_index()\n",
    "\n",
    "# Assuming procesgroep_pivot is sorted by 'teamcode' and 'datum'\n",
    "procesgroep_pivot = procesgroep_pivot.sort_values(by=['teamcode', 'datum'])\n",
    "\n",
    "# Apply the rolling sum function, avoiding 'teamcode' in the numeric operation\n",
    "def calculate_rolling_sums(group):\n",
    "    # Exclude 'teamcode' from the rolling operation\n",
    "    group = group.set_index('datum')\n",
    "    numeric_columns = group.select_dtypes(include=[np.number])\n",
    "    rolled = numeric_columns.rolling(window='7D', closed='left').sum()\n",
    "    # Include 'teamcode' and 'datum' back\n",
    "    rolled = rolled.reset_index()\n",
    "    \n",
    "    rolled['teamcode'] = group['teamcode'].iloc[0]  # Safe because 'group' is grouped by 'teamcode'\n",
    "    return rolled\n",
    "\n",
    "# Group by 'teamcode' and apply the rolling sum calculation\n",
    "rolling_sums = procesgroep_pivot.groupby('teamcode', as_index=False).apply(calculate_rolling_sums).reset_index(drop=True)\n",
    "\n",
    "# # Ensure 'datum' is in the correct datetime format after manipulations\n",
    "rolling_sums['datum'] = pd.to_datetime(rolling_sums['datum'])\n",
    "\n",
    "# # Merge the rolling sums with the daily_work DataFrame to align with the target variable\n",
    "daily_work_with_exog = pd.merge(daily_work, rolling_sums, on=['teamcode', 'datum'], how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0fb247-2c04-4033-b77b-5ebbd8502c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each type_regeling per day for each team\n",
    "type_regeling_counts = filtered_backlog.groupby(['teamcode', 'datum', 'type_regeling']).size().reset_index(name='count')\n",
    "type_regeling_pivot = type_regeling_counts.pivot_table(index=['teamcode', 'datum'], columns='type_regeling', values='count', fill_value=0).reset_index()\n",
    "\n",
    "# Ensure the dataframe is sorted\n",
    "type_regeling_pivot = type_regeling_pivot.sort_values(by=['teamcode', 'datum'])\n",
    "\n",
    "# Apply the rolling sum function to the type_regeling pivot\n",
    "rolling_sums_type_regeling = type_regeling_pivot.groupby('teamcode', as_index=False).apply(calculate_rolling_sums).reset_index(drop=True)\n",
    "\n",
    "# Ensure 'datum' is in the correct datetime format after manipulations\n",
    "rolling_sums_type_regeling['datum'] = pd.to_datetime(rolling_sums_type_regeling['datum'])\n",
    "\n",
    "# Merge the new rolling sums with the previously prepared daily_work_with_exog DataFrame\n",
    "daily_work_with_exog = pd.merge(daily_work_with_exog, rolling_sums_type_regeling, on=['teamcode', 'datum'], how='left', suffixes=('', '_type_regeling')).fillna(0)\n",
    "\n",
    "# # The suffixes parameter adds a suffix to overlapping column names except for the joining keys (teamcode and datum here)\n",
    "pivot = daily_work_with_exog.pivot(index='datum', columns='teamcode', values=[column for column in daily_work_with_exog.columns if column not in ('teamcode', 'datum')]).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6b30a8-e12f-4549-9ef3-eced13c1154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataFrame to store AIC, BIC, and parameters\n",
    "results_df = pd.DataFrame(columns=['Team', 'AIC', 'BIC', 'Order', 'Seasonal_Order'])\n",
    "metrics_df = pd.DataFrame(columns=['Team', 'MSE', 'MAE', 'R2'])\n",
    "\n",
    "# Updated time series cross-validation function\n",
    "def time_series_cv(data, exog_data, pdq, seasonal_pdq, splits):\n",
    "    results = []\n",
    "    tscv = TimeSeriesSplit(n_splits=splits)\n",
    "    param_results = []\n",
    "\n",
    "    for param in pdq:\n",
    "        for param_seasonal in seasonal_pdq:\n",
    "            fold_aic = []\n",
    "            fold_bic = []\n",
    "\n",
    "            for train_index, test_index in tscv.split(data):\n",
    "                train, test = data.iloc[train_index], data.iloc[test_index]\n",
    "                train_exog, test_exog = exog_data.iloc[train_index], exog_data.iloc[test_index]\n",
    "\n",
    "                try:\n",
    "                    model = SARIMAX(train,\n",
    "                                    exog=train_exog,\n",
    "                                    order=param,\n",
    "                                    seasonal_order=param_seasonal,\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False)\n",
    "                    model_fit = model.fit(disp=0)\n",
    "                    fold_aic.append(model_fit.aic)\n",
    "                    fold_bic.append(model_fit.bic)\n",
    "                except Exception as e:\n",
    "                    print(f\"An error occurred with parameters {param}, {param_seasonal}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            if fold_aic:  # Ensure there's at least one valid result\n",
    "                avg_aic = np.mean(fold_aic)\n",
    "                avg_bic = np.mean(fold_bic)\n",
    "                param_results.append({\n",
    "                    'Order': param,\n",
    "                    'Seasonal_Order': param_seasonal,\n",
    "                    'AIC': avg_aic,\n",
    "                    'BIC': avg_bic\n",
    "                })\n",
    "                print(f\"Evaluated parameters {param} and {param_seasonal}: AIC = {avg_aic}, BIC = {avg_bic}\")\n",
    "\n",
    "    # Find the best parameters based on the lowest average AIC\n",
    "    best_result = min(param_results, key=lambda x: x['AIC'])\n",
    "    best_order = best_result['Order']\n",
    "    best_seasonal_order = best_result['Seasonal_Order']\n",
    "\n",
    "    return param_results, best_order, best_seasonal_order\n",
    "\n",
    "# Assume pivot_daily_work and daily_work_with_exog are defined somewhere\n",
    "validation_start_date = pd.Timestamp('2024-02-19')\n",
    "\n",
    "# Loop over each team and process SARIMAX models\n",
    "for team in ['Zuid']:\n",
    "    print(f\"Processing team: {team}\")\n",
    "\n",
    "    # Example data extraction; replace with your actual data logic\n",
    "    team_series = pivot.loc[:, ('total_work_hours', team)].asfreq('D').fillna(0)\n",
    "    team_exog = pivot.loc[:, ([column for column in daily_work_with_exog.columns if column not in ('teamcode', 'datum', 'total_work_hours')], team)].asfreq('D').fillna(0)\n",
    "\n",
    "    # Split the data into training/testing and validation sets\n",
    "    train_test_series = team_series[:validation_start_date - pd.Timedelta(days=1)]\n",
    "    train_test_exog = team_exog[:validation_start_date - pd.Timedelta(days=1)]\n",
    "    validate_series = team_series[validation_start_date:]\n",
    "    validate_exog = team_exog[validation_start_date:]\n",
    "\n",
    "    # Parameter ranges (adjust as necessary based on earlier discussion)\n",
    "    p = q = P = Q  range(1, 7)\n",
    "    d = D = range(0, 2)\n",
    "    m = 7  # Weekly seasonality\n",
    "\n",
    "    # Generate all different combinations of p, d, q and P, D, Q\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "    seasonal_pdq = [(x[0], x[1], x[2], m) for x in list(itertools.product(P, D, Q))]\n",
    "\n",
    "    # Execute the cross-validation\n",
    "    team_results, best_order, best_seasonal_order = time_series_cv(train_test_series, train_test_exog, pdq, seasonal_pdq, splits=3)\n",
    "\n",
    "    # Append results to DataFrame with the team label\n",
    "    for result in team_results:\n",
    "        result['Noord'] = team  # Add team name to each result\n",
    "        results_df = pd.concat([results_df, pd.DataFrame([result])], ignore_index=True)\n",
    "\n",
    "    print(f\"Completed processing for team: {team}\")\n",
    "\n",
    "    # Fit the model with the best parameters\n",
    "    best_model = SARIMAX(train_test_series,\n",
    "                         exog=train_test_exog,\n",
    "                         order=best_order,\n",
    "                         seasonal_order=best_seasonal_order,\n",
    "                         enforce_stationarity=False,\n",
    "                         enforce_invertibility=False)\n",
    "    results = best_model.fit()\n",
    "    \n",
    "    # Summary of the model\n",
    "    print(results.summary())\n",
    "\n",
    "    # Diagnostic plots\n",
    "    results.plot_diagnostics(figsize=(15, 12))\n",
    "    plt.title(f'SARIMAX Diagnostics for {team}')\n",
    "    plt.savefig(f'Images/Diagnostics_{team}.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Forecast the validation period\n",
    "    forecast = results.get_forecast(steps=len(validate_series), exog=validate_exog)\n",
    "    forecast_mean = forecast.predicted_mean\n",
    "    forecast_ci = forecast.conf_int()\n",
    "\n",
    "    # Placeholder for the target DataFrame\n",
    "    target_df = None\n",
    "\n",
    "    # Iterate through each DataFrame in the list\n",
    "    for df in weekly_schedules:\n",
    "        # Check if the first row of the \"Datum\" column matches the desired date\n",
    "        if df.loc[0, \"Datum\"] == validation_start_date:\n",
    "            target_df = df\n",
    "            break\n",
    "\n",
    "    # Now, target_df holds the DataFrame of interest or None if not found\n",
    "    if target_df is not None:\n",
    "        target_df = target_df[~target_df[\"Dag\"].isin([\"zaterdag\", \"zondag\"])]\n",
    "        target_df = target_df[target_df[\"Team\"] != \"OD\"]\n",
    "\n",
    "        # Prepare the plotting data\n",
    "        actual_hours_per_day = filtered_backlog.groupby(['teamcode', 'datum'])['Normtijd (in hours)'].sum().reset_index()\n",
    "        actual_hours_per_day = actual_hours_per_day[(actual_hours_per_day['datum'] >= validation_start_date) & (actual_hours_per_day['datum'] <= validation_start_date + pd.Timedelta(days=11))]\n",
    "\n",
    "        target_df = target_df[~target_df[\"Dag\"].isin([\"zaterdag\", \"zondag\"])]\n",
    "        target_df['datum'] = pd.to_datetime(target_df['Datum'])  # Ensure 'Datum' in target_df is datetime\n",
    "        forecasts = round(forecast_mean[~forecast_mean.index.weekday.isin([5, 6])], 0).clip(0)  # 5 and 6 correspond to Saturday and Sunday, round and set negative to 0\n",
    "\n",
    "        team_target = target_df[target_df['Team'] == team]\n",
    "        team_target = team_target.set_index('datum').sort_index()\n",
    "\n",
    "        dates = team_target.index.strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "        team_forecasts = forecasts.reindex(team_target.index, fill_value=0)\n",
    "        team_conf_int = forecast_ci.reindex(team_target.index, fill_value=0)\n",
    "\n",
    "        # Filtering actual hours for the current team\n",
    "        team_actual_hours = actual_hours_per_day[actual_hours_per_day['teamcode'] == team]\n",
    "        team_actual_hours = team_actual_hours.set_index('datum').reindex(dates, fill_value=0)\n",
    "\n",
    "        team_target_filtered = team_target[team_target.index.isin(dates)]\n",
    "\n",
    "        lower_bounds = team_conf_int.iloc[:, 0].clip(lower=0)\n",
    "        upper_bounds = team_conf_int.iloc[:, 1]\n",
    "\n",
    "        # Calculate validation metrics\n",
    "        mse = mean_squared_error(team_actual_hours['Normtijd (in hours)'], team_forecasts)\n",
    "        mae = mean_absolute_error(team_actual_hours['Normtijd (in hours)'], team_forecasts)\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame([{\n",
    "            'Team': team,\n",
    "            'MSE': mse,\n",
    "            'MAE': mae\n",
    "        }])], ignore_index=True)\n",
    "\n",
    "        # Plotting for the current team\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        ax.plot(dates, team_target_filtered['TotalHours'], color='red', marker='o', linestyle='-', linewidth=2, markersize=5, label='Available Working Hours')\n",
    "        ax.plot(dates, team_forecasts, color='green', marker='x', linestyle='--', linewidth=2, markersize=5, label=f'Forecasted Hours for {team}')\n",
    "        ax.bar(dates, team_actual_hours['Normtijd (in hours)'], width=0.4, label=f'Actual Hours for {team}', align='center', color='purple')\n",
    "        ax.fill_between(dates, lower_bounds, upper_bounds, color='green', alpha=0.3, label='Confidence Interval')\n",
    "\n",
    "        # Set the x-axis ticks to the dates and label them with the corresponding dates\n",
    "        ax.set_xticks(dates)\n",
    "        ax.set_xticklabels(dates, rotation=45)\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Hours')\n",
    "        ax.set_title(f'Scheduled vs Target Hours for Team {team}')\n",
    "        ax.legend(loc='upper right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('model_selection_results.csv', index=False)\n",
    "print(\"Results saved to model_selection_results.csv\")\n",
    "\n",
    "# Save validation metrics to CSV\n",
    "metrics_df.to_csv('validation_metrics.csv', index=False)\n",
    "print(\"Validation metrics saved to validation_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8728e540-ed2d-4451-b2c9-41b79858a243",
   "metadata": {},
   "source": [
    "## Plot predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419cc25-4f5f-43b2-a339-7f3cee5bca99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get specefic schedule\n",
    "\n",
    "# Placeholder for the target DataFrame\n",
    "target_df = None\n",
    "\n",
    "# Iterate through each DataFrame in the list\n",
    "for df in weekly_schedules:\n",
    "    # Check if the first row of the \"Datum\" column matches the desired date\n",
    "    if df.loc[0, \"Datum\"] == start_date:\n",
    "        target_df = df\n",
    "        break\n",
    "\n",
    "# Now, target_df holds the DataFrame of interest or None if not found\n",
    "if target_df is not None:\n",
    "    None\n",
    "else:\n",
    "    print(\"DataFrame with the specified date not found.\")\n",
    "    \n",
    "target_df = target_df[~target_df[\"Dag\"].isin([\"zaterdag\", \"zondag\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e949429-d9cc-4f4d-8e01-da7c6f6e22c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get real schedule\n",
    "actual_hours_per_day = filtered_backlog.groupby(['teamcode', 'datum'])['Normtijd (in hours)'].sum().reset_index()\n",
    "actual_hours_per_day = actual_hours_per_day[(actual_hours_per_day['datum'] >= start_date) & (actual_hours_per_day['datum'] <= start_date + timedelta(days=11))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b34f0a-4827-4010-8655-12a69f6f14b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Prepare the Target Hours Data\n",
    "target_df['datum'] = pd.to_datetime(target_df['Datum'])  # Ensure 'Datum' in target_df is datetime\n",
    "\n",
    "forecasts = round(forecasts[~forecasts.index.weekday.isin([5, 6])], 0).clip(0)  # 5 and 6 correspond to Saturday and Sunday, round and set negative to 0\n",
    "\n",
    "# Iterate over each team to plot\n",
    "for team in actual_hours_per_day['teamcode'].unique():\n",
    "    team_target = target_df[target_df['Team'] == team]\n",
    "\n",
    "    # Preparing data for plotting\n",
    "    team_target = team_target.set_index('datum').sort_index()\n",
    "    \n",
    "    dates = team_target.index.strftime('%Y-%m-%d').tolist()\n",
    "\n",
    "    team_forecasts = forecasts[team].reindex(team_target.index, fill_value=0)\n",
    "    team_conf_int = conf_int_forecasts[team].reindex(team_target.index, fill_value=0)\n",
    "\n",
    "    # Filtering actual hours for the current team\n",
    "    team_actual_hours = actual_hours_per_day[actual_hours_per_day['teamcode'] == team]\n",
    "    team_actual_hours = team_actual_hours.set_index('datum').reindex(dates, fill_value=0)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        \n",
    "    team_target_filtered = team_target[team_target.index.isin(dates)]\n",
    "\n",
    "    # Target (Available) Hours Line Plot\n",
    "    ax.plot(dates, team_target_filtered['TotalHours'], color='red', marker='o', linestyle='-', linewidth=2, markersize=5, label='Available Working Hours')\n",
    "    ax.plot(dates, team_forecasts, color='green', marker='x', linestyle='--', linewidth=2, markersize=5, label='Forecasted Hours')\n",
    "    \n",
    "    lower_bounds = team_conf_int.iloc[:, 0].clip(lower=0)\n",
    "    upper_bounds = team_conf_int.iloc[:, 1]\n",
    "    \n",
    "    # Plot with adjusted confidence intervals\n",
    "    ax.fill_between(dates, lower_bounds, upper_bounds, color='green', alpha=0.3, label='Confidence Interval')\n",
    "    \n",
    "    x_positions = range(len(dates))  # Numeric x positions for the dates\n",
    "    ax.bar([x for x in x_positions], team_actual_hours['Normtijd (in hours)'], width=0.4, label='Actual Hours', align='center', color='purple')\n",
    "    \n",
    "    # Set the x-axis ticks to the dates and label them with the corresponding dates\n",
    "    ax.set_xticks(dates)\n",
    "    ax.set_xticklabels(dates, rotation=45)\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Hours')\n",
    "    ax.set_title(f'Scheduled vs Target Hours for Team {team}')\n",
    "    ax.legend(loc='upper right')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show plot\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environ",
   "language": "python",
   "name": "environ"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
