{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31c0360e-4251-4df1-86e3-e9fbf9ee9184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from azure.identity import AzureCliCredential\n",
    "from azure.storage.filedatalake import DataLakeFileClient\n",
    "from azure.storage.filedatalake import DataLakeServiceClient\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "import re\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from pulp import LpProblem, LpMinimize, LpMaximize, LpVariable, lpSum, LpStatus, value, LpBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af754b48-6c56-4e41-aa0a-78576bf88e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_credential():\n",
    "    # Haal het token op wat met az login is aangemaakt\n",
    "    return AzureCliCredential()\n",
    "\n",
    "\n",
    "def read_file(credential, storage_account, container,\n",
    "                  filepath):\n",
    "    account_url = \"https://{}.dfs.core.windows.net\".format(storage_account)\n",
    "\n",
    "    file_client = DataLakeFileClient(account_url=account_url,\n",
    "                                     file_system_name=container,\n",
    "                                     file_path=filepath,\n",
    "                                     credential=credential)\n",
    "\n",
    "    downloaded_bytes = io.BytesIO(file_client.download_file().readall())\n",
    "    return downloaded_bytes\n",
    "\n",
    "\n",
    "def write_file(buffer, credential, storage_account, container,\n",
    "                filepath):\n",
    "    account_url = \"https://{}.dfs.core.windows.net\".format(storage_account)\n",
    "\n",
    "    file_client = DataLakeFileClient(account_url=account_url,\n",
    "                                     file_system_name=container,\n",
    "                                     file_path=filepath,\n",
    "                                     credential=credential)\n",
    "\n",
    "    file_client.upload_data(buffer.getvalue(), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ed8c04-62df-43b0-b1d6-068150db06f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Haal het token op wat met az login is aangemaakt\n",
    "credential = get_credential()\n",
    "\n",
    "# Definieer storage-account en containername\n",
    "storage_account_name = \"\"\n",
    "container_name = \"\"\n",
    "account_url = \"https://{}.dfs.core.windows.net\".format(storage_account_name)\n",
    "reports_folder = \"426_rapportage/\"\n",
    "schedule_folder = \"werkvoorraad_planning/\"\n",
    "schedule2_folder = \"werkverdeling_acceptatie/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d94af32-c8bd-4f28-ad94-cc5550b55e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def get_df(folder = \"426_rapportage/\", file_name = \"426_werkvoorraad_MO_20230131_AMB.xlsx\"):\n",
    "    file_path = folder + '/' + file_name\n",
    "\n",
    "    file_bytes = read_file(credential=credential,\n",
    "                  storage_account=storage_account_name,\n",
    "                  container=container_name,\n",
    "                  filepath=file_path)\n",
    "    \n",
    "    return(file_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87ea3c07-d440-437c-bce0-939c00b7fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_all_file_names(account_url, credential, container_name, folder):\n",
    "    # Get a reference to the file system and directory\n",
    "    service_client = DataLakeServiceClient(account_url=account_url, credential=credential)\n",
    "    file_system_client = service_client.get_file_system_client(container_name)\n",
    "\n",
    "    file_list = []\n",
    "    # List files in the directory\n",
    "    files = file_system_client.get_paths()\n",
    "    for file in files:\n",
    "        if ((file.name.startswith(folder)) & (\".xlsx\" in file.name)):\n",
    "            file_list.append(file.name)\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2626bbda-3158-4f02-a812-31cf4822bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract filename with the specific date\n",
    "def extract_filename_with_date(filenames, date):\n",
    "    for filename in filenames:\n",
    "        if date in filename:\n",
    "            return filename\n",
    "    return None  # Return None if no matching filename is found\n",
    "\n",
    "# Function to convert YYYYMMDD to DD-MM-YYYY\n",
    "def convert_date_format(yyyymmdd):\n",
    "    return f\"{yyyymmdd[6:8]}-{yyyymmdd[4:6]}-{yyyymmdd[0:4]}\"\n",
    "\n",
    "# Function to extract filename with the specific date, considering the new format\n",
    "def extract_filename_with_converted_date(filenames, date):\n",
    "    for filename in filenames:\n",
    "        # Use regular expressions to find the date in YYYYMMDD format within the filename\n",
    "        match = re.search(r'\\d{8}', filename)\n",
    "        if match:\n",
    "            date_in_file = match.group(0)  # Extract the date string\n",
    "            converted_date = convert_date_format(date_in_file)  # Convert to DD-MM-YYYY\n",
    "            if converted_date == date:\n",
    "                return filename"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd38467-ad55-417e-98b8-874a1a710148",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load data into environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "337da40d-9e08-41e5-8255-cb9a53ddb666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create an aggregated dataframe of the employee availability.\n",
    "def transform_schedule(df, skill_lookup):\n",
    "    weekly_schedule_df = df.copy()\n",
    "\n",
    "    # Identifying team names and their respective employee columns\n",
    "    team_columns = [col for col in weekly_schedule_df.columns if 'Team' in col]\n",
    "\n",
    "    # Mapping each team to its members\n",
    "    team_member_map = {}\n",
    "    for i, team_col in enumerate(team_columns):\n",
    "        team_idx = weekly_schedule_df.columns.get_loc(team_col)\n",
    "        if i < len(team_columns) - 1:\n",
    "            next_team_idx = weekly_schedule_df.columns.get_loc(team_columns[i + 1])\n",
    "            members = weekly_schedule_df.columns[team_idx + 1:next_team_idx]\n",
    "        else:\n",
    "            members = weekly_schedule_df.columns[team_idx + 1:]\n",
    "\n",
    "        team_member_map[weekly_schedule_df[team_col].iloc[0]] = members\n",
    "\n",
    "    # Transforming the dataframe\n",
    "    new_data = []\n",
    "    for index, row in weekly_schedule_df.iterrows():\n",
    "        for team_name, team_members in team_member_map.items():\n",
    "            total_hours = row[team_members].apply(pd.to_numeric, errors='coerce').sum()\n",
    "            all_skills_hours = 0\n",
    "            not_all_skills_hours = 0\n",
    "\n",
    "            for member in team_members:\n",
    "                first_name = member\n",
    "                hours = pd.to_numeric(row[first_name], errors='coerce')\n",
    "                if pd.isna(hours):\n",
    "                    hours = 0  # Set NaN values to 0\n",
    "                skill = skill_lookup.get(first_name, 'Acceptant A')  # Default to 'Acceptant A' if not found\n",
    "                \n",
    "                if skill == 'Acceptant B':\n",
    "                    not_all_skills_hours += hours\n",
    "                else:\n",
    "                    all_skills_hours += hours\n",
    "\n",
    "            new_data.append({\n",
    "                'Datum': row['Datum'],\n",
    "                'Dag': row['Dag'],\n",
    "                'Team': team_name,\n",
    "                'TotalHours': total_hours,\n",
    "                'AllSkillsHours': all_skills_hours,\n",
    "                'NotAllSkillsHours': not_all_skills_hours\n",
    "            })\n",
    "\n",
    "    # Create the new dataframe\n",
    "    transformed_df = pd.DataFrame(new_data)\n",
    "\n",
    "    # Display the first few rows of the new dataframe\n",
    "    return transformed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "404b099d-013a-4d80-b7c7-2bcfba81f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load in skill matrix and make lookup file\n",
    "skill_matrix = pd.read_excel(\"Skillmatrix.xlsx\", skiprows=2)\n",
    "\n",
    "def extract_first_name(full_name):\n",
    "    return full_name.split()[0]\n",
    "\n",
    "# Step 1: Extract first names and create a mapping from first name to full name and skill\n",
    "skill_matrix['FirstName'] = skill_matrix['Naam'].apply(extract_first_name)\n",
    "\n",
    "# Step 2: Create a dictionary for skill lookup\n",
    "skill_lookup = skill_matrix.set_index('FirstName')['Medewerkerprofiel'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "295a5bc2-7567-4529-bf38-560d2671c396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping file werkvoorraad_planning/oktober/11-10-2023 Werkverdeling Acceptatie.xlsx because 'ROOSTER' sheet doesn't exist.\n"
     ]
    }
   ],
   "source": [
    "#Get latest working schedule\n",
    "schedule_file_names = get_all_file_names(account_url, credential, container_name, folder = schedule_folder)\n",
    "schedule_file_names2 = get_all_file_names(account_url, credential, container_name, folder = schedule2_folder)\n",
    "\n",
    "weekly_schedules = []\n",
    "for i in schedule_file_names:\n",
    "    split_name = i.rsplit('/',1)\n",
    "    excel_file = get_df(folder=split_name[0] + '/', file_name=split_name[1])\n",
    "    # Check if the 'ROOSTER' sheet exists in the Excel file\n",
    "    if 'ROOSTER' in pd.ExcelFile(excel_file).sheet_names:\n",
    "        temp = pd.read_excel(excel_file, sheet_name='ROOSTER')\n",
    "        temp['Datum'] = pd.to_datetime(temp['Datum'], errors='coerce')\n",
    "        temp = temp.dropna(subset=['Datum'])\n",
    "        temp = temp.dropna(axis=1, how='all')\n",
    "        transformed_temp = transform_schedule(temp, skill_lookup)\n",
    "        weekly_schedules.append(transformed_temp)\n",
    "        # weekly_schedules.append(temp)\n",
    "    else:\n",
    "        print(f\"Skipping file {i} because 'ROOSTER' sheet doesn't exist.\")\n",
    "\n",
    "for i in schedule_file_names2:\n",
    "    split_name = i.rsplit('/',1)\n",
    "    excel_file = get_df(folder=split_name[0] + '/', file_name=split_name[1])\n",
    "    # Check if the 'ROOSTER' sheet exists in the Excel file\n",
    "    if 'ROOSTER' in pd.ExcelFile(excel_file).sheet_names:\n",
    "        temp = pd.read_excel(excel_file, sheet_name='ROOSTER')\n",
    "        temp['Datum'] = pd.to_datetime(temp['Datum'], errors='coerce')\n",
    "        temp = temp.dropna(subset=['Datum'])\n",
    "        temp = temp.dropna(axis=1, how='all')\n",
    "        transformed_temp = transform_schedule(temp, skill_lookup)\n",
    "        weekly_schedules.append(transformed_temp)\n",
    "    else:\n",
    "        print(f\"Skipping file {i} because 'ROOSTER' sheet doesn't exist.\")        \n",
    "\n",
    "schedule = pd.concat(weekly_schedules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "312d8ce0-712d-487a-b530-3b1936d9497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get task groups\n",
    "task_groups = pd.read_excel('Procesgroep taken acceptatie.xlsx', sheet_name ='Lijstvorm')\n",
    "task_groups = task_groups[['Procesgroep', 'Proces']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "122c6f1b-e03d-4832-ae97-e6a765c57b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get working backlog\n",
    "report_file_names = get_all_file_names(account_url, credential, container_name, folder = reports_folder)\n",
    "\n",
    "reports = []\n",
    "for i in report_file_names:\n",
    "    split_name = i.rsplit('/',1)\n",
    "    excel_file = get_df(folder=split_name[0] + '/', file_name=split_name[1])\n",
    "    date = split_name[1].rsplit('MO_', 1)[1].rsplit('_A')[0]\n",
    "    temp = pd.read_excel(excel_file)\n",
    "    temp['datum'] = pd.to_datetime(split_name[1].rsplit('MO_', 1)[1].rsplit('_A')[0], format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "    reports.append(temp)\n",
    "\n",
    "backlog = pd.concat(reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90dad0e-0a3f-42aa-b0be-3893bc11df31",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6aaa268e-ecc0-4b71-ae6b-73df61f57a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean working backlog\n",
    "backlog = backlog[backlog['contractnummer'].notna()]\n",
    "backlog['contractnummer'] =  backlog['contractnummer'].astype(np.int64)\n",
    "backlog['datum'] = pd.to_datetime(backlog['datum'])\n",
    "backlog = backlog.merge(task_groups, left_on='taaknaam', right_on='Proces', how='left')\n",
    "filtered_backlog = backlog.copy()\n",
    "# filtered_backlog = backlog[['datum', 'contractnummer', 'Procesgroep_y', 'taaknaam', 'taakomschrijving_aangepast', 'teamcode', 'Uitvoeren voor']]\n",
    "filtered_backlog.rename(columns={'Procesgroep_y': 'Procesgroep'}, inplace=True)\n",
    "filtered_backlog = filtered_backlog[filtered_backlog['Procesgroep'] != 'Quion']\n",
    "filtered_backlog['week_nummer'] = filtered_backlog['datum'].dt.isocalendar().week\n",
    "\n",
    "# Define the list of team codes you want to keep\n",
    "team_codes_to_keep = ['Zuid', 'Midden', 'Noord', 'IMD']\n",
    "\n",
    "# Use the isin method to filter the DataFrame\n",
    "filtered_backlog = filtered_backlog[filtered_backlog['teamcode'].isin(team_codes_to_keep)]\n",
    "filtered_backlog.loc[filtered_backlog['productlijn'] == 'Attens Hypotheek', 'teamcode'] = 'Attens'\n",
    "filtered_backlog = filtered_backlog[filtered_backlog[\"teamcode\"] != \"IMD\"]\n",
    "# Group by 'procesgroep' and 'contractnummer', then count unique 'taaknaam' for each group.\n",
    "unique_tasks_per_group_and_contract = filtered_backlog.groupby(['Procesgroep', 'contractnummer'])['Proces'].nunique().reset_index(name='unique_tasks')\n",
    "\n",
    "# Now, group by 'procesgroep' again to calculate the average number of unique tasks per procesgroep.\n",
    "average_unique_tasks_per_procesgroep = unique_tasks_per_group_and_contract.groupby('Procesgroep')['unique_tasks'].mean()\n",
    "\n",
    "# Define the column names\n",
    "column_names = ['Procesgroep', 'Normtijd (in minuten)']\n",
    "\n",
    "# Manually enter the rows with data\n",
    "rows = [\n",
    "    [\"Aanvragen\", 16.8],\n",
    "    [\"Rebound\", 21.0],\n",
    "    [\"1e fiat\", 55.8],\n",
    "    [\"2e fiat\", 27.0],\n",
    "    ['Afronding dossier', 10.0/2],\n",
    "    ['Schoningstaken', 10.0]\n",
    "]\n",
    "\n",
    "# Create the dataframe\n",
    "task_times = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "filtered_backlog = filtered_backlog.merge(task_times, left_on=\"Procesgroep\", right_on=\"Procesgroep\", how=\"left\")\n",
    "filtered_backlog['Normtijd (in minuten)'] = (filtered_backlog['Normtijd (in minuten)'] / 60).round(2)\n",
    "filtered_backlog.rename(columns={'Normtijd (in minuten)': 'Normtijd (in hours)'}, inplace=True)\n",
    "filtered_backlog['datum'] = filtered_backlog['datum'] + timedelta(days=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d4d3b5c-5b78-4c3d-8504-8b08171354c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean schedule\n",
    "schedule = schedule.drop_duplicates()\n",
    "schedule = schedule.fillna(0)\n",
    "\n",
    "# List of columns to exclude from replacement\n",
    "exclude_columns = ['Datum', 'Dag', 'Team']\n",
    "\n",
    "# List of columns representing employee hours\n",
    "employee_columns = [col for col in schedule.columns if col not in exclude_columns]\n",
    "\n",
    "# Apply the replacement only to the employee columns\n",
    "schedule[employee_columns] = schedule[employee_columns].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "team_codes_to_keep = ['Zuid', 'Midden', 'Noord', 'Attens']\n",
    "schedule = schedule[schedule['Team'].isin(team_codes_to_keep)]\n",
    "\n",
    "schedule = schedule.drop_duplicates(subset=['Datum', 'Dag', 'Team'], keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a05f680-0132-4820-805c-9ae6d3ba65c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your start and end date for the date range\n",
    "start_date = '06-12-2023'\n",
    "end_date = '2023-06-16'\n",
    "\n",
    "\n",
    "# filtered_backlog = filtered_backlog[(filtered_backlog['datum'] >= start_date) & (filtered_backlog['Uitvoeren voor'] <= end_date)]\n",
    "# Filter the dates within the specified range and exclude weekends\n",
    "filtered_backlog = filtered_backlog[\n",
    "    (filtered_backlog['datum'] >= start_date) & \n",
    "    (filtered_backlog['Uitvoeren voor'] <= end_date)  # Monday=0, Sunday=6\n",
    "]\n",
    "\n",
    "schedule = schedule[\n",
    "    (schedule['Datum'] >= start_date) & \n",
    "    (schedule['Datum'] <= end_date) &\n",
    "    (schedule['Datum'].dt.weekday < 5)  # Monday=0, Sunday=6\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6918ffc2-5f20-4e46-b97a-171d2c517e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort tasks by deadline ('Uitvoeren voor')\n",
    "filtered_backlog.sort_values(by='Uitvoeren voor', inplace=True)\n",
    "backlog_copy = filtered_backlog.copy()\n",
    "\n",
    "# Initialize lists to track scheduled and unscheduled tasks\n",
    "unscheduled_tasks_list = []\n",
    "scheduled_tasks_list = []\n",
    "\n",
    "# Iterate over each unique team in the schedule\n",
    "for team in schedule['Team'].unique():\n",
    "    team_schedule = schedule[schedule['Team'] == team]\n",
    "\n",
    "    # Iterate over each day's schedule for the team\n",
    "    for _, day_info in team_schedule.iterrows():\n",
    "        date = day_info['Datum']\n",
    "        total_available_hours = day_info['TotalHours']\n",
    "        all_skills_hours = day_info['AllSkillsHours']\n",
    "        not_all_skills_hours = day_info['NotAllSkillsHours']\n",
    "\n",
    "        # Filter tasks that can be potentially scheduled on this date for this team\n",
    "        possible_tasks = backlog_copy[(backlog_copy['teamcode'] == team) & \n",
    "                                          (backlog_copy['datum'] <= date)]\n",
    "\n",
    "        # Iterate over the tasks that can be scheduled on this date\n",
    "        for task_index, task in possible_tasks.iterrows():\n",
    "            task_hours = task['Normtijd (in hours)']\n",
    "            procesgroep = task['Procesgroep']\n",
    "\n",
    "            if procesgroep == '2e fiat':\n",
    "                # Task requires AllSkillsHours\n",
    "                if task_hours <= all_skills_hours:\n",
    "                    # Schedule the task\n",
    "                    scheduled_tasks_list.append(task)\n",
    "                    all_skills_hours -= task_hours\n",
    "                    total_available_hours -= task_hours\n",
    "\n",
    "                    # Remove the task from backlog\n",
    "                    backlog_copy.drop(task_index, inplace=True)\n",
    "                else:\n",
    "                    # Check if the task can be deferred\n",
    "                    next_possible_dates = team_schedule[team_schedule['Datum'] > date]\n",
    "                    if not next_possible_dates.empty and task['Uitvoeren voor'] >= next_possible_dates.iloc[0]['Datum']:\n",
    "                        # Task can be considered in next dates\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Task cannot be scheduled at all\n",
    "                        unscheduled_tasks_list.append(task)\n",
    "                        backlog_copy.drop(task_index, inplace=True)\n",
    "            else:\n",
    "                # Task can use either AllSkillsHours or NotAllSkillsHours\n",
    "                if task_hours <= total_available_hours and (task_hours <= all_skills_hours or task_hours <= not_all_skills_hours):\n",
    "                    # Schedule the task\n",
    "                    scheduled_tasks_list.append(task)\n",
    "                    if task_hours <= all_skills_hours:\n",
    "                        all_skills_hours -= task_hours\n",
    "                    else:\n",
    "                        not_all_skills_hours -= task_hours\n",
    "                    total_available_hours -= task_hours\n",
    "\n",
    "                    # Remove the task from backlog\n",
    "                    backlog_copy.drop(task_index, inplace=True)\n",
    "                else:\n",
    "                    # Check if the task can be deferred\n",
    "                    next_possible_dates = team_schedule[team_schedule['Datum'] > date]\n",
    "                    if not next_possible_dates.empty and task['Uitvoeren voor'] >= next_possible_dates.iloc[0]['Datum']:\n",
    "                        # Task can be considered in next dates\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Task cannot be scheduled at all\n",
    "                        unscheduled_tasks_list.append(task)\n",
    "                        backlog_copy.drop(task_index, inplace=True)\n",
    "\n",
    "# Convert the list of dictionaries to a DataFrame\n",
    "scheduled_tasks = pd.DataFrame(scheduled_tasks_list)\n",
    "unscheduled_tasks = pd.DataFrame(unscheduled_tasks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c242da-ca84-4a44-a12d-904858345b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate scheduled task hours by day and team\n",
    "scheduled_hours_per_day = scheduled_tasks.groupby(['teamcode', 'datum'])['Normtijd (in hours)'].sum().reset_index()\n",
    "\n",
    "# Check if unscheduled_tasks is not empty before aggregating\n",
    "if not unscheduled_tasks.empty:\n",
    "    unscheduled_hours_per_day = unscheduled_tasks.groupby(['teamcode', 'datum'])['Normtijd (in hours)'].sum().reset_index()\n",
    "else:\n",
    "    unscheduled_hours_per_day = pd.DataFrame()  # Create an empty DataFrame if unscheduled_tasks is empty\n",
    "\n",
    "# Convert 'datum' to a more convenient format (e.g., YYYY-MM-DD) if necessary\n",
    "scheduled_hours_per_day['datum'] = pd.to_datetime(scheduled_hours_per_day['datum']).dt.strftime('%Y-%m-%d')\n",
    "if not unscheduled_hours_per_day.empty:\n",
    "    unscheduled_hours_per_day['datum'] = pd.to_datetime(unscheduled_hours_per_day['datum']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Iterate over each team to plot\n",
    "for team in scheduled_tasks['teamcode'].unique():\n",
    "    team_schedule = schedule[schedule['Team'] == team].copy()  # Make a copy here    \n",
    "    team_scheduled = scheduled_hours_per_day[scheduled_hours_per_day['teamcode'] == team]\n",
    "    if not unscheduled_hours_per_day.empty:\n",
    "        team_unscheduled = unscheduled_hours_per_day[unscheduled_hours_per_day['teamcode'] == team]\n",
    "    else:\n",
    "        team_unscheduled = pd.DataFrame(columns=['teamcode', 'datum', 'Normtijd (in hours)'])  # Create an empty DataFrame with columns for compatibility\n",
    "    \n",
    "    # Ensure 'Datum' in team_schedule is converted to datetime for accurate plotting\n",
    "    team_schedule['Datum'] = pd.to_datetime(team_schedule['Datum'])\n",
    "\n",
    "    # Create a date range that includes all days, filling the gaps (e.g., weekends)\n",
    "    all_dates = pd.date_range(start=team_schedule['Datum'].min(), end=team_schedule['Datum'].max())\n",
    "\n",
    "    # Ensure 'datum' in team_scheduled and team_unscheduled is converted to datetime\n",
    "    team_scheduled['datum'] = pd.to_datetime(team_scheduled['datum'])\n",
    "    if not team_unscheduled.empty:\n",
    "        team_unscheduled['datum'] = pd.to_datetime(team_unscheduled['datum'])\n",
    "\n",
    "    # Merge scheduled and unscheduled dataframes with all_dates to ensure all dates are included\n",
    "    all_dates_df = pd.DataFrame(all_dates, columns=['Datum'])\n",
    "    combined_hours = pd.merge(all_dates_df, team_scheduled, left_on='Datum', right_on='datum', how='left')\n",
    "    combined_hours = pd.merge(combined_hours, team_unscheduled, on='datum', how='outer', suffixes=('_scheduled', '_unscheduled')).fillna(0)\n",
    "\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    # Scheduled task hours with turquoise color\n",
    "    ax.bar(combined_hours['Datum'], combined_hours['Normtijd (in hours)_scheduled'], width=0.4, label='Scheduled Hours', align='center', color='tab:blue')\n",
    "\n",
    "    # Unscheduled task hours\n",
    "    if not unscheduled_hours_per_day.empty:\n",
    "        ax.bar(combined_hours['Datum'], combined_hours['Normtijd (in hours)_unscheduled'], width=0.4, label='Unscheduled Hours', align='center', color='orange', bottom=combined_hours['Normtijd (in hours)_scheduled'])\n",
    "\n",
    "    # Available hours with gaps for weekends or non-working days\n",
    "    available_hours_mask = team_schedule['TotalHours'] > 0\n",
    "    ax.plot(team_schedule.loc[available_hours_mask, 'Datum'], team_schedule.loc[available_hours_mask, 'TotalHours'], color='red', marker='o', linestyle='-', linewidth=2, markersize=8, label='Available Hours')\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Hours')\n",
    "    ax.set_title(f'Task Schedule for Team: {team}')\n",
    "    ax.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'Images/HeuristicSchedule{team}.png')\n",
    "    # Show plot\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "environ",
   "language": "python",
   "name": "environ"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
